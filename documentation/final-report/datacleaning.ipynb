{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a991831d",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "BTRY 4100 -- Final Project\n",
    "\n",
    "Andrew Chung, hc893"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef293a",
   "metadata": {},
   "source": [
    "## Station Ridership Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "# download_url = \"https://data.ny.gov/api/views/5wq4-mkjj/rows.csv?accessType=DOWNLOAD\"\n",
    "# hourly ridership (station-level)\n",
    "hourly_ridership_url = \"5wq4-mkjj\"\n",
    "\n",
    "def export_from(url):\n",
    "  return pd.read_csv(\"{}{}{}\".format(\n",
    "    \"https://data.ny.gov/api/views/\",\n",
    "    url,\n",
    "    \"/rows.csv?accessType=DOWNLOAD\"\n",
    "  ))\n",
    "\n",
    "def impute(data, \n",
    "          gc_shuttle = 'S 42nd', \n",
    "          fk_shuttle = 'S Fkln',\n",
    "          jz = 'J'):\n",
    "  df = data.copy()\n",
    "  df.loc[df['line'] == gc_shuttle, 'line'] = 'SG'\n",
    "  df.loc[df['line'] == fk_shuttle, 'line'] = 'SF'\n",
    "  df.loc[df['line'] == jz, 'line'] = 'JZ' # merge J/Z\n",
    "  return df\n",
    "\n",
    "def extract_lines(text):\n",
    "  matches = re.findall(r'\\((.*?)\\)', text)\n",
    "  items = np.concatenate([item.split(',') for item in matches])\n",
    "   # sometimes, station names in parentheses get thrown in the mix\n",
    "  return ','.join(map(str, items[np.char.str_len(items) <= 2]))\n",
    "\n",
    "def main():\n",
    "\n",
    "  print(\"Loading data files from data.ny.gov...\")\n",
    "  print(datetime.now())\n",
    "  # read in the data: hourly ridership\n",
    "  hourly_ridership_data = export_from(hourly_ridership_url).query(\n",
    "    \"transit_mode == \\'subway\\'\"\n",
    "  ).reset_index().drop(columns = ['index'])\n",
    "  hourly_ridership_data['transit_timestamp'] = pd.to_datetime(hourly_ridership_data['transit_timestamp'])\n",
    "\n",
    "  print(\"Hourly Ridership data successfully loaded.\")\n",
    "  print(datetime.now())\n",
    "\n",
    "  print(\"Processing hourly ridership data:\")\n",
    "\n",
    "  # cleaning station data\n",
    "\n",
    "  # define peak time blocks\n",
    "  start_time_am = pd.to_datetime('06:30:00').time()\n",
    "  end_time_am = pd.to_datetime('09:30:00').time()\n",
    "  start_time_pm = pd.to_datetime('15:30:00').time()\n",
    "  end_time_pm = pd.to_datetime('20:00:00').time()\n",
    "\n",
    "  # first, filter by month (Jan-Feb)\n",
    "  hourly_ridership_data = hourly_ridership_data[\n",
    "    hourly_ridership_data['transit_timestamp'].dt.month < 3\n",
    "  ]\n",
    "  # filter hourly ridership data by peak status\n",
    "  hourly_ridership_data = hourly_ridership_data[\n",
    "    hourly_ridership_data['transit_timestamp'].dt.time.between(start_time_am, end_time_am) |\n",
    "    hourly_ridership_data['transit_timestamp'].dt.time.between(start_time_pm, end_time_pm)\n",
    "  ].sort_values(by = 'transit_timestamp').reset_index().drop(columns = ['index'])\\\n",
    "                                      .groupby(['transit_timestamp', 'station_complex'], as_index = False)\\\n",
    "                                      .agg({\n",
    "                                        'borough': lambda x: x.mode()[0], # stations do not span different boroughs\n",
    "                                        'ridership': 'sum'\n",
    "                                      })\n",
    "  \n",
    "  # remove weekends\n",
    "  hourly_ridership_data = hourly_ridership_data[hourly_ridership_data['transit_timestamp'].dt.weekday < 5] # 5,6 are Sat/Sun\n",
    "  # remove holidays\n",
    "  hourly_ridership_data = hourly_ridership_data[~hourly_ridership_data['transit_timestamp'].dt.date.isin(pd.to_datetime([\n",
    "    '2025-01-01', '2025-01-20', '2025-02-17'\n",
    "  ]))]\n",
    "\n",
    "  # group by station ID, aggregate hourly ridership into monthly figures\n",
    "  hourly_ridership_data['month'] = hourly_ridership_data['transit_timestamp'].dt.month\n",
    "  monthly_ridership_data = hourly_ridership_data.groupby(['station_complex', 'month'], as_index = False).agg({\n",
    "    'borough': lambda x: x.mode()[0],\n",
    "    'ridership': 'sum'\n",
    "  })\n",
    "  monthly_ridership_data['lines'] = monthly_ridership_data['station_complex'].apply(extract_lines)\n",
    "  monthly_ridership_data.to_csv(\"MTA_Subway_Ridership_Summarized_Apr21.csv\", index = False)\n",
    "\n",
    "  print(\"Hourly Ridership Dataset successfully written into .csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae681c46",
   "metadata": {},
   "source": [
    "## Line Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317121e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "# download_url = \"https://data.ny.gov/api/views/5wq4-mkjj/rows.csv?accessType=DOWNLOAD\"\n",
    "# subway line-level data sets\n",
    "on_time_url = \"ks33-g5ze\"\n",
    "trains_delayed_url = \"9zbp-wz3y\"\n",
    "service_delivered_url = \"nmu4-7tz9\"\n",
    "late_arrivals_url = \"x7nj-r656\"\n",
    "major_incidents_url = \"uqnw-2qfk\"\n",
    "customer_journey_url = \"s4u6-t435\"\n",
    "wait_assessment_url = \"62c4-mvcx\"\n",
    "terminal_ontime_url = \"ks33-g5ze\"\n",
    "\n",
    "def export_from(url):\n",
    "  return pd.read_csv(\"{}{}{}\".format(\n",
    "    \"https://data.ny.gov/api/views/\",\n",
    "    url,\n",
    "    \"/rows.csv?accessType=DOWNLOAD\"\n",
    "  ))\n",
    "\n",
    "def impute(data, \n",
    "          gc_shuttle = 'S 42nd', \n",
    "          fk_shuttle = 'S Fkln',\n",
    "          jz = 'J'):\n",
    "  df = data.copy()\n",
    "  df.loc[df['line'] == gc_shuttle, 'line'] = 'SG'\n",
    "  df.loc[df['line'] == fk_shuttle, 'line'] = 'SF'\n",
    "  df.loc[df['line'] == jz, 'line'] = 'JZ' # merge J/Z\n",
    "  return df\n",
    "\n",
    "def extract_lines(text):\n",
    "  matches = re.findall(r'\\((.*?)\\)', text)\n",
    "  items = np.concatenate([item.split(',') for item in matches])\n",
    "   # sometimes, station names in parentheses get thrown in the mix\n",
    "  return ','.join(map(str, items[np.char.str_len(items) <= 2]))\n",
    "\n",
    "def main():\n",
    "\n",
    "  print(\"Loading data files from data.ny.gov...\")\n",
    "\n",
    "  # subway line performance data\n",
    "  ## customer journey metrics\n",
    "  customer_journey_data = export_from(customer_journey_url).query(\n",
    "    \"period == \\\"peak\\\" & line not in ['W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['division', 'index', 'period'])\n",
    "  customer_journey_data = impute(customer_journey_data).groupby(['line', 'month']).agg({\n",
    "    'num_passengers': 'sum', # sum over 2 months\n",
    "    'additional platform time': 'mean', \n",
    "    'additional train time': 'mean', \n",
    "    'over_five_mins_perc': 'mean'\n",
    "  }).reset_index().rename(columns = {\n",
    "    'index': 'line',\n",
    "    'additional platform time': 'additional_platform_time',\n",
    "    'additional train time': 'additional_train_time'\n",
    "  })\n",
    "\n",
    "  ## wait assessment\n",
    "  wait_assessment_data = export_from(wait_assessment_url).query(\n",
    "    \"day_type == 1 and period == \\\"peak\\\" & line not in ['H', 'W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['division', 'index', 'day_type', 'period'])\n",
    "  wait_assessment_data = impute(wait_assessment_data, gc_shuttle = 'GS', fk_shuttle = 'FS').groupby(['line', 'month']).agg({\n",
    "    'wait assessment': 'mean'\n",
    "  }).reset_index().rename(columns = {'index': 'line', 'wait assessment': 'wait_assessment'})\n",
    "\n",
    "  ## service delivered\n",
    "  service_delivered_data = export_from(service_delivered_url).query(\n",
    "    \"day_type == 1 & line not in ['H', 'W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['division', 'index', 'day_type'])\n",
    "  service_delivered_data = impute(service_delivered_data, gc_shuttle = 'GS', fk_shuttle = 'FS').groupby(['line', 'month']).agg({\n",
    "    'service delivered': 'mean'\n",
    "  }).reset_index().rename(columns = {'index': 'line', 'service delivered': 'service_delivered'})\n",
    "\n",
    "  ## terminal on-time performance\n",
    "  terminal_ontime_data = export_from(terminal_ontime_url).query(\n",
    "    \"day_type == 1 & line not in ['W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['division', 'index', 'day_type'])\n",
    "  terminal_ontime_data = impute(terminal_ontime_data).groupby(['line', 'month']).agg({\n",
    "    'terminal_on_time_performance': 'mean'\n",
    "  }).reset_index().rename(columns = {'index': 'line'})\n",
    "\n",
    "  # 4-5 minute late arrivala\n",
    "  late_arrivals_data = export_from(late_arrivals_url).query(\n",
    "    \"`Day Type` == 1 & Line not in ['SI', 'W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['Division', 'index', 'Day Type']).rename(columns = {\n",
    "    'Month': 'month',\n",
    "    'Line': 'line',\n",
    "    'Percent Late': 'percent_late'\n",
    "  })\n",
    "  late_arrivals_data = impute(\n",
    "    late_arrivals_data[late_arrivals_data['month']\\\n",
    "      .isin(['2025-01-01', '2025-02-01'])]\\\n",
    "      .reset_index()\\\n",
    "      .drop(columns = ['index'])\n",
    "  )\n",
    "  late_arrivals_data.loc[late_arrivals_data['line'] == 'NW', 'line'] = 'N' # NW -> N\n",
    "  late_arrivals_data = late_arrivals_data.groupby(['line', 'month']).agg({\n",
    "    'percent_late': 'mean'\n",
    "  }).reset_index().rename(columns = {'index': 'line'})\n",
    "\n",
    "  ## trains delayed\n",
    "  trains_delayed_data = export_from(trains_delayed_url).query(\n",
    "    \"day_type == 1 & line not in ['W', 'S Rock']\"\n",
    "  ).reset_index().drop(columns = ['division', 'index', 'day_type'])\n",
    "  trains_delayed_data = impute(trains_delayed_data, gc_shuttle = 'GS')\n",
    "\n",
    "  ## major incidents\n",
    "  major_incidents_data = export_from(major_incidents_url).query(\n",
    "    \"day_type == 1\"\n",
    "  ).dropna().reset_index().drop(columns = ['index']).dropna()\n",
    "\n",
    "  print(\"All data files loaded.\")\n",
    "  print(\"Merging First 5 datasets...\")\n",
    "\n",
    "  # aggregate line-specific data\n",
    "\n",
    "  # lines in the NYC subway system\n",
    "  subway_lines = np.concatenate((\n",
    "    np.arange(1,8).astype(str), # numbered lines\n",
    "    np.array([\n",
    "      \"SG\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"JZ\",\"L\",\"M\",\"N\",\"Q\",\"R\",\"SF\"\n",
    "    ]) # lettered lines\n",
    "  ))\n",
    "\n",
    "  # initialize dataset, assign lines and divisions\n",
    "  line_data = pd.DataFrame(columns =  ['line', 'month']).assign(line = subway_lines, division = None)\n",
    "  datasets = [\n",
    "    customer_journey_data, \n",
    "    wait_assessment_data,\n",
    "    service_delivered_data, \n",
    "    terminal_ontime_data,\n",
    "    late_arrivals_data\n",
    "  ]\n",
    "\n",
    "  for dataset in range(len(datasets)):\n",
    "    assert 'line' in datasets[dataset].columns, \"line does not exist in {}\".format(dataset)\n",
    "\n",
    "  # Join Datasets\n",
    "  line_data = reduce(lambda left, right: pd.merge(left, right, on = ['line', 'month'], how = 'left'), datasets)\n",
    "  line_data\n",
    "\n",
    "  print(\"Merging major incidents and train delay data...\")\n",
    "\n",
    "  # major delays and trains delayed data\n",
    "  # major incidents: indicator variables\n",
    "  # I will group the incidents into 2 types\n",
    "  ## 1. Infrastructural -- signal malfunction, subway car, track, stations and structural\n",
    "  ## 2. Personal/civil: Persons on trackbed/police/medical, other\n",
    "  incidents = major_incidents_data['category'].unique()\n",
    "  major_incidents_data['class'] = major_incidents_data['category'].map({\n",
    "    'Signals': 'Infrastructure',\n",
    "    'Subway Car': 'Infrastructure',\n",
    "    'Track': 'Infrastructure',\n",
    "    'Stations and Structure': 'Infrastructure',\n",
    "    'Persons on Trackbed/Police/Medical': 'Non-Infrastructure',\n",
    "    'Other': 'Non-Infrastructure'\n",
    "  })\n",
    "  incident_data = major_incidents_data.groupby(['line', 'month', 'class']).agg({\n",
    "    'count': 'sum'\n",
    "  }).reset_index().pivot_table(\n",
    "    index = ['line', 'month'], \n",
    "    columns = 'class', \n",
    "    values = 'count', \n",
    "    aggfunc ='sum'\n",
    "  ).reset_index().rename(columns = {\n",
    "    'index': 'line',\n",
    "    'Infrastructure': 'infra_critical',\n",
    "    'Non-Infrastructure': 'noninfra_critical'\n",
    "  }).fillna(0)\n",
    "  incident_data['infra_critical'] = incident_data['infra_critical'].astype('Int64')\n",
    "  incident_data['noninfra_critical'] = incident_data['noninfra_critical'].astype('Int64')\n",
    "\n",
    "  # Delays: in similar fashion, except the reports are already categorized.\n",
    "  # These events have not spurred major incidents but have nonetheless slowed service.\n",
    "  ## 1. Infrastructural: Crew Availability, Infra/Equipment, Operating Conditions, Planned ROW work\n",
    "  ## 2. Non-Infrastructural: Police & Medical, External Factors\n",
    "  delays = trains_delayed_data['reporting_category'].unique()\n",
    "  trains_delayed_data['class'] = trains_delayed_data['reporting_category'].map({\n",
    "    'Crew Availability': 'Infrastructure',\n",
    "    'Infrastructure & Equipment': 'Infrastructure',\n",
    "    'Operating Conditions': 'Infrastructure',\n",
    "    'Planned ROW Work': 'Infrastructure',\n",
    "    'External Factors': 'Non-Infrastructure',\n",
    "    'Police & Medical': 'Non-Infrastructure'\n",
    "  })\n",
    "  delay_data = trains_delayed_data.groupby(['line', 'month', 'class']).agg({\n",
    "    'delays': 'sum'\n",
    "  }).reset_index().pivot_table(\n",
    "    index = ['line', 'month'],\n",
    "    columns = 'class',\n",
    "    values = 'delays',\n",
    "    aggfunc = 'sum'\n",
    "  ).reset_index().rename(columns = {\n",
    "    'index': 'line',\n",
    "    'Infrastructure': 'infra_noncritical',\n",
    "    'Non-Infrastructure': 'noninfra_noncritical'\n",
    "  }).fillna(0)\n",
    "  delay_data['infra_noncritical'] = delay_data['infra_noncritical'].astype('Int64')\n",
    "  delay_data['noninfra_noncritical'] = delay_data['noninfra_noncritical'].astype('Int64')\n",
    "\n",
    "  line_data = line_data.merge(\n",
    "    incident_data, on = ['line', 'month'], how = 'left'\n",
    "  ).merge(\n",
    "    delay_data, on = ['line', 'month'], how = 'left'\n",
    "  ).fillna(0) # note there is no existing data for major incidents in shuttle services.\n",
    "  \n",
    "  line_data.to_csv(\"MTA_Subway_Line_Data_2025_Apr21.csv\")\n",
    "\n",
    "  print(\"Line Dataset successfully written into .csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea1713",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def impute_shuttles(data):\n",
    "  df = data.copy()\n",
    "  df = df[~df['lines'].isin(['A,S'])] # remove Rockaway shuttle stations\n",
    "  # since shuttle type is not specified in hourly ridership data, I will\n",
    "  # manually impute it based on domain knowledge.\n",
    "\n",
    "  # SG (42nd St Shuttle) connects to Times Square (1,2,3,7,A,C,E,N,Q,R,W) and Grand Central (4,5,6,7)\n",
    "  # SF (Franklin Av Shuttle) connects to Franklin Av (A,C), Botanic Garden (2,3,4,5), and Prospect Park (B,Q); Park Place (no connections)\n",
    "\n",
    "  def assign_lines(val):\n",
    "    parts = [p.strip() for p in val.split(',')]\n",
    "    sg_stations = np.array([\"N,Q,R,W,S,1,2,3,7,A,C,E\", \"S,4,5,6,7\"])\n",
    "    sf_stations = np.array([\"2,3,4,5,S\", \"C,S\", \"B,Q,S\", \"S\"])\n",
    "\n",
    "    if val in sg_stations:\n",
    "      return ','.join(['SG' if p == 'S' else p for p in parts])\n",
    "    elif val in sf_stations:\n",
    "      return ','.join(['SF' if p == 'S' else p for p in parts])\n",
    "    else:\n",
    "      return val\n",
    "\n",
    "  df['lines'] = df['lines'].apply(assign_lines)\n",
    "  return df\n",
    "\n",
    "def main():\n",
    "  \n",
    "  print(\"Processing Monthly Ridership data...\")\n",
    "  # fix shuttle designations\n",
    "  # aggregate by line complexes\n",
    "  ridership_data = pd.read_csv(\n",
    "    \"MTA_Subway_Ridership_Summarized_Apr21.csv\"\n",
    "  ).groupby(['lines', 'month'], as_index = False).agg({\n",
    "    'ridership': 'mean'\n",
    "  })\n",
    "  ridership_data = impute_shuttles(ridership_data)\n",
    "  \n",
    "  print(\"Processing line performance data...\")\n",
    "\n",
    "  # import line performanace data\n",
    "  line_data = pd.read_csv(\"MTA_Subway_Line_Data_2025_Apr21.csv\").iloc[:, 1:] # remove index column\n",
    "  line_data['month'] = pd.to_datetime(line_data['month']).dt.month\n",
    "  metrics = line_data.columns[3:].to_numpy()\n",
    "  # compute weights by gross ridership\n",
    "  line_data['line_weight'] = line_data['num_passengers']/line_data['num_passengers'].sum()\n",
    "\n",
    "  # assign metrics to ridership data, name it \"composite_data_unweighted\"\n",
    "  # no weights for relative subway ridership applied\n",
    "  composite_data_unweighted = ridership_data.assign(**{col: None for col in metrics})\n",
    "\n",
    "  # assign weighted metrics to ridership data, name it \"composite_data_weighted\"\n",
    "  # weights for relative subway ridership computed by ridership numbers\n",
    "  composite_data_weighted = ridership_data.assign(**{col: None for col in np.append(metrics, ['line_weight'])})\n",
    "\n",
    "  # helper function to resolve confusions with N/W and J/Z lines\n",
    "  def impute_lines(lines):\n",
    "    arr = lines.copy().tolist()\n",
    "    # Case 1: N/W\n",
    "    if 'W' in arr:\n",
    "      if 'N' not in arr:\n",
    "        arr[arr == 'W'] = 'N'\n",
    "      else:\n",
    "        arr.remove('W')\n",
    "\n",
    "    # Case 2: J/Z\n",
    "    if 'J' in arr:\n",
    "      if 'Z' in arr:\n",
    "        arr.remove('J')\n",
    "        arr.remove('Z')\n",
    "        arr.append('JZ')\n",
    "      else:\n",
    "        arr = ['JZ' if x == 'J' else x for x in arr]\n",
    "\n",
    "    return np.array(arr)\n",
    "\n",
    "  print(\"Computing unweighted and weighted composite datasets...\")\n",
    "\n",
    "  # fill in performance data (unweighted)\n",
    "  for idx, row in composite_data_unweighted.iterrows():\n",
    "\n",
    "    lines_arr = impute_lines(np.array(row['lines'].split(',')))\n",
    "    row['lines'] = ','.join(lines_arr) # fix line formatting\n",
    "    month = row['month']\n",
    "    indices = line_data[\n",
    "      (line_data['month'] == month) & (line_data['line'].isin(lines_arr))\n",
    "    ].index.to_numpy()\n",
    "\n",
    "    for metric in metrics:\n",
    "      subframe = line_data.loc[indices]\n",
    "      weights_sum = subframe['line_weight'].sum()\n",
    "      composite_data_unweighted.loc[idx, metric] = subframe[metric].sum()\n",
    "      composite_data_weighted.loc[idx, metric] = (\n",
    "        subframe[metric] * (subframe['line_weight'] * len(indices)/weights_sum)\n",
    "      ).sum()\n",
    "\n",
    "  # save to .csv\n",
    "  composite_data_unweighted.to_csv(\"Unweighted_Data_Apr22.csv\")\n",
    "  composite_data_weighted.to_csv(\"Weighted_Data_Apr22.csv\")\n",
    "\n",
    "  print(\"Successfully saved to .csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
